{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica de José Luis Jerez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://4c887c19028a:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1523832854347)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_date: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Account_Created: string (nullable = true)\n",
      " |-- Last_Login: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType, TimestampType}\n",
       "data: org.apache.spark.rdd.RDD[String] = datasetPracticaFinal/TransaccionesNew.csv MapPartitionsRDD[1] at textFile at <console>:30\n",
       "customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Transaction_date,StringType,true), StructField(Product,StringType,true), StructField(Price,IntegerType,true), StructField(Payment_Type,StringType,true), StructField(Name,StringType,true), StructField(City,StringType,true), StructField(Account_Created,StringType,true), StructField(Last_Login,StringType,true), StructField(Latitude,DoubleType,true), StructField(Longitude,DoubleType,true), StructField(Description,..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType, TimestampType}\n",
    "\n",
    "// Carga de dataset en RDD - Spark Core\n",
    "val data = sc.textFile(\"datasetPracticaFinal/TransaccionesNew.csv\")\n",
    "\n",
    "//Carga de dataset en DataFrame - SparkSQL\n",
    "//val df = spark.sqlContext.read.format(\"csv\")\n",
    "//  .option(\"header\", \"true\")\n",
    "//  .option(\"inferSchema\", \"true\")\n",
    "//  .load(\"datasetPracticaFinal/TransaccionesNew.csv\")\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    // StructField(\"Transaction_date\", TimestampType, true),\n",
    "    StructField(\"Transaction_date\", StringType, true),\n",
    "    StructField(\"Product\", StringType, true),\n",
    "    StructField(\"Price\", IntegerType, true),\n",
    "    StructField(\"Payment_Type\", StringType, true),\n",
    "    StructField(\"Name\", StringType, true),\n",
    "    StructField(\"City\", StringType, true),\n",
    "    StructField(\"Account_Created\", StringType, true),\n",
    "    StructField(\"Last_Login\", StringType, true),\n",
    "    StructField(\"Latitude\", DoubleType, true),\n",
    "    StructField(\"Longitude\", DoubleType, true),\n",
    "    StructField(\"Description\", StringType, true)))\n",
    "\n",
    "val df = spark.sqlContext.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(customSchema)\n",
    "  .load(\"datasetPracticaFinal/TransaccionesNew.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "// Registra el dataframe en una tabla con el nombre pasado por parámetro\n",
    "df.createOrReplaceTempView(\"transacciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 1: \n",
    "  + Definición de objeto transacción\n",
    "    + DNI (Este dato no viene en el dataset, se generará con un acumulador de forma incremental (1 a N)\n",
    "    + Tarjeta de crédito\n",
    "    + Importe\n",
    "    + Descripción del pago\n",
    "    + Ciudad\n",
    "    + Categoría\n",
    "  + Definición objeto cliente:DNI\n",
    "    + Nombre\n",
    "    + Número de cuenta\n",
    "    + Balance\n",
    "    + Tarjeta de crédito\n",
    "    + Ciudad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario definir previamente el objeto \"geolocalizacion\"\n",
    "- Definición de objeto geolocalizacion\n",
    "    + Latitud\n",
    "    + Longitud\n",
    "    + Ciudad\n",
    "    + Pais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Geolocalizacion\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Geolocalizacion(latitud: Double, longitud: Double, ciudad: String, pais: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Transaccion\n",
       "defined class Cliente\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Transaccion(DNI: Long, longitud: Double, categoria: String, tarjetaCredito: String, geolocalizacion: Geolocalizacion)\n",
    "case class Cliente(DNI: Long, nombre: String, cuenaCorriente: String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementa las siguientes categorizaciones usando funciones Core de Spark.\n",
    "Los resultados se almacenarán combinando diferentes formatos de ficheros (CSV, Avro, Parquet etc)\n",
    "\n",
    "- Tarea 2: Agrupa todos los clientes por ciudad.El objetivo sería contar todos las transacciones ocurridas por ciudad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at filter at <console>:34\n",
       "tarea2: scala.collection.Map[String,Long] = Map(Drogheda -> 1, Saint Catharines -> 1, Bunbury -> 1, Tel Aviv -> 1, Amsterdam -> 3, Ehingen an der Donau -> 1, Yellowknife -> 1, Navan -> 1, \"Spring Lake Park            \" -> 1, Danderyd -> 1, Lucca -> 1, \"Atlanta                     \" -> 2, Killiney -> 1, \"Stamford                    \" -> 1, \"Roanoke                     \" -> 1, \"Chula Vista                 \" -> 1, \"Larchmont                   \" -> 1, \"New Rochelle                \" -> 2, Stocksund -> 1, Scamander -> 1, \"Atchison                    \" -> 1, \"Temecula                    \" -> 1, Oxford -> 3, Calne -> 1, Telgte -> 1, \"Swampscott                  \" -> 1, Waterford -> 2, \"Pacific Palisades    ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Utilizando RDDs\n",
    "\n",
    "val words = data.map(_.split(',')(5)).filter( _ != \"ciudad\" )\n",
    "val tarea2 = words.countByValue\n",
    "sc.parallelize(tarea2.toSeq).saveAsTextFile(\"tarea2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 3: Encuentra aquellos clientes que hayan realizado pagos superiores a 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[16] at map at <console>:34\n",
       "cabecera: (String, String) = (Price,Name)\n",
       "rddSinCabecera: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[17] at filter at <console>:40\n",
       "tarea3: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[18] at filter at <console>:42\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Utilizando RDDs\n",
    "\n",
    "val c = data.map(line => {\n",
    "  val lines = line.split(',')\n",
    "  (lines(2), lines(4))\n",
    "})\n",
    "c.first.getClass\n",
    "val cabecera = c.first()\n",
    "val rddSinCabecera = c.filter(row => !row.equals(cabecera))\n",
    "// val rddSinCabecera2 = rddSinCabecera.map(fila => (fila._1.toInt,fila._2))\n",
    "val tarea3 = rddSinCabecera.filter(fila => fila._1.toInt > 500)\n",
    "tarea3.toDF.write.csv(\"tarea3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|             Name|Price|\n",
      "+-----------------+-----+\n",
      "|         carolina| 1200|\n",
      "|           Betina| 1200|\n",
      "|Federica e Andrea| 1200|\n",
      "|            Gouya| 1200|\n",
      "|          Gerd W | 3600|\n",
      "|         LAURENCE| 1200|\n",
      "|            Fleur| 1200|\n",
      "|             adam| 1200|\n",
      "|  Renee Elisabeth| 1200|\n",
      "|            Aidan| 1200|\n",
      "|            Stacy| 1200|\n",
      "|            Heidi| 1200|\n",
      "|            Sean | 1200|\n",
      "|          Georgia| 1200|\n",
      "|          Richard| 1200|\n",
      "|           Leanne| 1200|\n",
      "|            Janet| 1200|\n",
      "|          barbara| 1200|\n",
      "|           Sabine| 3600|\n",
      "|             Hani| 1200|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tarea3_Spark_sql: org.apache.spark.sql.DataFrame = [Name: string, Price: int]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Utilizando DataFrames\n",
    "\n",
    "val tarea3_Spark_sql = spark.sql(\"SELECT Name, Price FROM transacciones WHERE Price > 500\")\n",
    "tarea3_Spark_sql.show\n",
    "tarea3.toDF.write.parquet(\"tarea3_Spark_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 4: Obtén todas las transacciones agrupadas por cliente cuya la la ciudad X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     City|     Name|\n",
      "+---------+---------+\n",
      "|Amsterdam|Antonella|\n",
      "|Amsterdam| Caterina|\n",
      "|Amsterdam|    sunny|\n",
      "+---------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:630)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:241)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)",
      "  ... 39 elided",
      ""
     ]
    }
   ],
   "source": [
    "val tarea4 = spark.sql(\"SELECT City, Name FROM transacciones WHERE City = 'Amsterdam' ORDER BY Name\")\n",
    "tarea4.show\n",
    "//tarea4.toDF.write.csv(\"tarea4\")\n",
    "tarea4.toDF.write.format(\"com.databricks.spark.avro\").save(\"tarea4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 5: Filtra todas las operaciones cuya categoría sea Ocio (Restaurant, Cinema y Sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-----+------------+-----------------+--------------------+---------------+-------------+-----------+----------+-----------+\n",
      "|Transaction_date| Product|Price|Payment_Type|             Name|                City|Account_Created|   Last_Login|   Latitude| Longitude|Description|\n",
      "+----------------+--------+-----+------------+-----------------+--------------------+---------------+-------------+-----------+----------+-----------+\n",
      "|     1/2/09 4:53|Product1| 1200|        Visa|           Betina|Parkville        ...|            124|  1/2/09 7:49|     39.195| -94.68194| Restaurant|\n",
      "|    1/2/09 13:08|Product1| 1200|  Mastercard|Federica e Andrea|Astoria          ...|            125| 1/3/09 12:32|   46.18806|   -123.83|     Cinema|\n",
      "|    1/3/09 14:44|Product1| 1200|        Visa|            Gouya|              Echuca|            126| 1/3/09 14:22|-36.1333333|    144.75|     Sports|\n",
      "|    1/8/09 16:24|Product1| 1200|        Visa|         jennifer|Phoenix          ...|            160| 1/8/09 18:30|   33.44833|-112.07333|     Cinema|\n",
      "|     1/9/09 6:39|Product1| 1200|  Mastercard|           Anneli|Houston          ...|            161|  1/9/09 7:11|   29.76306| -95.36306|     Sports|\n",
      "|     1/7/09 7:44|Product1| 1200|  Mastercard|            Marie|Ball Ground      ...|            164| 1/9/09 10:52|   34.33806| -84.37667|     Cinema|\n",
      "|    1/3/09 13:24|Product1| 1200|        Visa|     Mehmet Fatih|           Helsingor|            165| 1/9/09 11:14| 56.0333333|12.6166667|     Sports|\n",
      "|    1/3/09 10:11|Product2| 3600|        Visa|      Christiane |Delray Beach     ...|            168| 1/10/09 9:46|   26.46111| -80.07306|     Cinema|\n",
      "|    1/9/09 15:58|Product1| 1200|  Mastercard|             Sari|             Newbury|            169|1/10/09 13:16|       51.4|-1.3166667|     Sports|\n",
      "|   1/10/09 14:43|Product1| 1200|      Diners|           Anupam|            Kinsaley|            172|1/10/09 14:37| 53.4247222|-6.1758333|     Cinema|\n",
      "|   1/10/09 12:05|Product1| 1200|        Visa|           Karina|Fort Lauderdale  ...|            173|1/10/09 16:33|   26.12194| -80.14361|     Sports|\n",
      "|    1/7/09 10:01|Product1| 1200|        Visa|           Darren|Pittsboro        ...|            176|1/10/09 20:02|      35.72|  -79.1775|     Cinema|\n",
      "|     1/1/09 1:26|Product1| 1200|  Mastercard|            Nikki|New Rochelle     ...|            177|1/10/09 21:31|   40.91139| -73.78278|     Sports|\n",
      "|   1/10/09 21:38|Product1| 1200|        Visa|          Anushka|Maple Ridge Distr...|            180|1/11/09 19:32|      49.25|    -122.5|     Cinema|\n",
      "|     1/7/09 6:18|Product1| 1200|  Mastercard|            June |Beachwood        ...|            181|1/11/09 19:35|   41.46444| -81.50889|     Sports|\n",
      "|     1/2/09 6:07|Product1| 1200|        Visa|           Cindy |              Kemble|            184| 1/12/09 2:24| 51.6766667|-2.0180556|     Cinema|\n",
      "|    1/12/09 3:25|Product1| 1200|  Mastercard|          chrissy|W Lebanon        ...|            185| 1/12/09 3:22|   43.64917| -72.31083|     Sports|\n",
      "|    1/12/09 5:18|Product1| 1200|  Mastercard|        Bernadett|         Southampton|            188| 1/12/09 8:08|       50.9|      -1.4|     Cinema|\n",
      "|    1/8/09 15:16|Product1| 1200|        Visa|           Dottie|Woodsboro        ...|            189| 1/12/09 9:29|   39.53306|   -77.315|     Sports|\n",
      "|    1/2/09 22:00|Product1| 1200|      Diners|            Lynne|Memphis          ...|            192|1/12/09 13:18|   35.14944| -90.04889|     Cinema|\n",
      "+----------------+--------+-----+------------+-----------------+--------------------+---------------+-------------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tarea5: org.apache.spark.sql.DataFrame = [Transaction_date: string, Product: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tarea5 = spark.sql(\"SELECT * FROM transacciones Where Description = 'Restaurant' OR Description = 'Cinema' OR Description = 'Sports'\")\n",
    "tarea5.show\n",
    "tarea5.toDF.write.json(\"tarea5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 6: Obtén las últimas transacciones de cada cliente en los últimos 30 días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+------------+--------------------+\n",
      "|                 td|Transaction_date|        Name|                City|\n",
      "+-------------------+----------------+------------+--------------------+\n",
      "|2009-01-15 01:41:00|    1/15/09 1:41|       Karen|           Roquefort|\n",
      "|2009-01-15 02:40:00|    1/15/09 2:40|      Alexis|Genoa            ...|\n",
      "|2009-01-15 03:43:00|    1/15/09 3:43|   Stephanie|               Rouen|\n",
      "|2009-01-15 04:12:00|    1/15/09 4:12|   catherine|Keller           ...|\n",
      "|2009-01-15 05:11:00|    1/15/09 5:11|         Pam|              London|\n",
      "|2009-01-15 05:27:00|    1/15/09 5:27|     Derrick|           North Bay|\n",
      "|2009-01-15 05:47:00|    1/15/09 5:47|Dave and Amy|Norfolk          ...|\n",
      "|2009-01-15 07:26:00|    1/15/09 7:26|      Celina|            Den Haag|\n",
      "|2009-01-15 08:07:00|    1/15/09 8:07|     Kirstie|Royal Oak        ...|\n",
      "|2009-01-15 08:10:00|    1/15/09 8:10|      Scott |Royal Oak        ...|\n",
      "|2009-01-15 08:12:00|    1/15/09 8:12|   Katherine|Marietta         ...|\n",
      "|2009-01-15 09:00:00|    1/15/09 9:00|      Cormac|       Freeport City|\n",
      "|2009-01-15 09:52:00|    1/15/09 9:52|        Mark|Holmdel          ...|\n",
      "|2009-01-15 10:16:00|   1/15/09 10:16|       Karin|Olive Branch     ...|\n",
      "|2009-01-15 10:24:00|   1/15/09 10:24|      Grace |                York|\n",
      "|2009-01-15 10:37:00|   1/15/09 10:37|       lamia|Little Compton   ...|\n",
      "|2009-01-15 10:51:00|   1/15/09 10:51|        Pia |                Sola|\n",
      "|2009-01-15 11:26:00|   1/15/09 11:26|      Nicole|Brookline        ...|\n",
      "|2009-01-15 11:43:00|   1/15/09 11:43|      Maggie|Libertyville     ...|\n",
      "|2009-01-15 11:45:00|   1/15/09 11:45|      Simone|             Watford|\n",
      "+-------------------+----------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.unix_timestamp\n",
       "df2: org.apache.spark.sql.DataFrame = [Transaction_date: string, Product: string ... 9 more fields]\n",
       "td: org.apache.spark.sql.Column = CAST(unix_timestamp(Transaction_date, MM/dd/yy HH:mm) AS TIMESTAMP)\n",
       "tarea6: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [td: timestamp, Transaction_date: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp\n",
    "val df2 = df.toDF(\"Transaction_date\",\"Product\",\"Price\",\"Payment_Type\",\"Name\",\"City\",\"Account_Created\",\"Last_Login\",\"Latitude\",\"Longitude\",\"Description\")\n",
    "val td = unix_timestamp($\"Transaction_date\", \"MM/dd/yy HH:mm\").cast(\"timestamp\")\n",
    "val tarea6 = df2.withColumn(\"td\", td).select($\"td\", $\"Transaction_date\", $\"Name\", $\"City\")\n",
    "   .filter($\"td\" > \"2009-01-15 00:00:00\")\n",
    "   .orderBy($\"td\".asc)\n",
    "tarea6.show\n",
    "tarea6.write.csv(\"tarea6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tarea 7[Opcional]: Encuentra por cada transacción a través de su longitud y latitud cual es el país a donde pertenece la transacción y guardalo en el campo país."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@throws(classOf[java.io.IOException])\n",
    "@throws(classOf[java.net.SocketTimeoutException])\n",
    "def get(url: String,\n",
    "        connectTimeout: Int = 5000,\n",
    "        readTimeout: Int = 5000,\n",
    "        requestMethod: String = \"GET\") =\n",
    "{\n",
    "    import java.net.{URL, HttpURLConnection}\n",
    "    val connection = (new URL(url)).openConnection.asInstanceOf[HttpURLConnection]\n",
    "    connection.setConnectTimeout(connectTimeout)\n",
    "    connection.setReadTimeout(readTimeout)\n",
    "    connection.setRequestMethod(requestMethod)\n",
    "    val inputStream = connection.getInputStream\n",
    "    val content = io.Source.fromInputStream(inputStream).mkString\n",
    "    if (inputStream != null) inputStream.close\n",
    "    content\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object GetRestContent\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object GetRestContent extends App {\n",
    "\n",
    "  val url = \"http://api.hostip.info/get_json.php?ip=12.215.42.19\"\n",
    "  val result = scala.io.Source.fromURL(url).mkString\n",
    "  println(result)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetRestContent.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
